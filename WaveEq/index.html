<html>
<head>
	<link rel="stylesheet" type="text/css" href="../index.css">
	<META name="keywords" content="Quazi, TowerClimb, Tower Climb, game, gamedev, indie, engineer, engineering, programming, Kevin"> 
	<title>Real-Time Wave Tracing</title>
		<style>
			body { margin: 10; }
			canvas { width: 1028px; height: 512px }
		</style>
	
</head>
<script src="../js/QZMath.js"></script>
<body>

<div id="wrap">
<h1>Real-Time Wave Tracing</h1>
</div>
<canvas id="c"></canvas>
<br>
<div id="wrap">
<div style="width: 100%; display: table;">
    <div style="display: table-row;">
        <div id="slidersA" style="width: 50%; display: table-cell;">
        </div>
        <div id="slidersB" style="display: table-cell;">
        </div>
    </div>
</div>

<br>
The text accompanying this page will hopefully get some updates in the future, but I wanted to get around to uploading this so people could play with it. In particular, try changing the "averaging filter constant", "white intensity", and ("steps per frame" to control the simulation speed according to what your hardware supports).
<br>
<br>
This started out as me wanting to make an interactive tool to visualize how chromatic aberration arises in lenses. At first I thought this could be relatively straightforward, just do some ray tracing with various wavelengths and wavelength dependent properties right? Then I started thinking about how it might be cool to also simulate diffraction effects, and this led me down a rabbit hole... diffraction is a lot trickier than just tracing rays. I've played around a lot with the classic wave simulation stuff (<a href="https://www.gdcvault.com/play/203/Fast-Water-Simulation-for-Games">[Fast Water Simulation for Games Using Height Fields]</a>) you see in a lot of games/demos, which is just a super simple PDE solver taking the Laplacian on a grid of values to compute updates to a velocity (Keenan Crane has a great lecture on the Laplace operator that you should watch, it really helped me understand things better <a href="https://www.youtube.com/watch?v=oEq9ROl9Umk">[video]</a>). I figured why not use the classic wave equation directly to simulate light. I was already pretty sure this wouldn't be too physically accurate but I wanted to see what it would actually look like and how far I could get with the idea before I ran into problems (I find this is always a good way to learn new things). So a caveat before you read more, I'm not an expert on this stuff and you need to do something smarter (finite-difference time-domain method) to properly account for all the stuff going on in Maxwell's equations. 
<br>
<br>
Anyway, I figured I could use the standard wave equation and vary the wave speed of a material region to get interfaces for refraction effects. A light source would be akin to some kind of sinusoidal pumping region which moved at a specific frequency. This worked as expected, giving reflection, bounces, etc. But my visualization was pretty... boring. I was showing the amplitude of the waves and that's pretty much it:
<br>
<br>
<img src="amp.png" alt="amplitude viz" class="displayed" id="big_img">
<br>
<br>
What you can see here: there's a light source, some interesting standing waves since this is all happening in a small cavity, and a little lens in the lower right focusing some waves. I wanted to actually display the proper color of light sources dependent on their frequency. This led me down yet another rabbit hole. The obvious thing to do is just set the color based on the frequency of light at a particular point right? Well pause for a moment and think - how do you get that frequency? My first thought was to "use the Fourier transform" to get the frequency. Well after this "dumb" thought I noticed the obvious problems with it. The Fourier transform gives a basically incomprehensible image which looks nothing like the spatial version. We need the frequency at specific locations. The Fourier transform is also stupidly slow so you need to process the whole image. I didn't like this option, so I researched a bit and found out wavelet transforms are a thing. You can essentially convolve your spatial data with spatially limited "wavelets" to get information about the "approximate" frequency at a specific region. This sounded a lot more shader friendly, and the "spatially limited" nature of the wavelets meant I would get something that was like a blurrier version of my image. But after trying this out, I got this image and had to think some more:
<br>
<br>
<img src="wavelet.png" alt="wavelet viz" class="displayed" id="big_img">
<br>
<br>
Sure it looks cool, but it's not really what I wanted to achieve. This is a picture of a *single* wavelength source with some kind of wavelet transform applied to get an approximation of the "frequency" at each point in order to color it (I think I used a Gabor wavelet). What I liked is that the colors were now smoothly varying and not "wavy" like the amplitude viz. One problem though, wavelet transforms don't actually give a precise frequency, it's more of an arbitrary value dependent on your wavelets. You can sort of remap this to frequencies but it's not a precise/exact value. Another issue which I thought I could find a solution for: the frequency/wavelength you measure has an orientation dependence. You can see that high frequencies (blue) exist in the X direction from the point source, and low frequencies in the Y direction, to the point it's mostly black past a certain angle. This is because my wavelet was sampling in the X direction. I figured I could do some kind of separation in X and Y and combine them somehow, but this didn't work out and gave muddy results (but a pretty picture at least!):
<br>
<br>
<img src="mixedwave.png" alt="XY wavelet viz" class="displayed" id="big_img">
<br>
<br>
I was pretty unhappy, wavelets were obviously a dead end and I should have noticed something key before trying them out. This kind of led to a small breakthrough though. I was forcing myself to find the frequency using spatial information, and completely forgot about the possibility of determining the frequency at each point using the time-domain signal. After a lot of thinking I came up with what I think was a really clever solution: I imagined I could simulate tuned mass spring dampers at every single pixel, and these could then be forced by the amplitude of the underlying wave. mass spring dampers have the nice property that they resonate (build up energy) when they're forced at a particular specific frequency. They essentially filter out anything which isn't near their tuned frequency. I imagined getting a few of these and then using this to approximate the frequency for each pixel from the total mechanical energy of the spring damper. Then I had a small epiphany, our eyes sort of work in the same way, photons fly in, excite some molecules on our rods and cones, and this excitation is converted to what we actually see. I figured I could actually match the response of three tuned mass spring dampers at each pixel to the L-cones, M-cones and S-cones which give us color vision, so I went down yet another rabbit hole learning about how to get proper RGB values from LMS excitations (thank you http://www.cvrl.org/, I find it funny this website is ironically kinda hard on the eyes given it's dedicated to vision research). Using some data from there I wrote a little program in python using pytorch for optimization to find mass, spring and damping constants for 3 tuned mass spring dampers in order to get their steady state energy when forced at particular frequencies to match the curves matching LMS response at particular frequencies. You can see this optimization running below:
<br>
<br>
<video class="displayed" controls autoplay loop>
  <source src="optimize.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
<br>
<br>
I also added a non-linear transform to the energy computation with some extra constants to stretch the responses in the right places and get a nicer fit. Actually coding this into a shader ended up being way easier than the wavelet stuff. I was essentially able to build off the existing simulation for the waves and add in a simple simulation for each spring damper's state which runs in parallel to the wave sim at each pixel. What you end up looking at in the sim above are the energies of a sea of little tuned oscillators riding on the underlying wave. The entire field is the surface a big flat eye covered in cones. Obviously this solution isn't without issues, in particular overly intense light and frequency extremes are converted into the "wrong" colors, but you kind of have to wonder how wrong this actually is. Extremely intense light at any frequency shone into your eye or some sensor would also wind up looking wrong, and would also burn/destroy those objects eventually. It's a bit of an idealization in the first place that specific wavelengths map to specific colors of the rainbow, our eyes contain molecules that can break down, and don't behave so differently to the tuned mass spring dampers, but of course their response at the extremes isn't really considered here or realistically matched.
<br>
<br>
The last thing I needed to figure out was how to get dispersion (color splitting in a prism). It turns out this arises due to interactions of charges in transparent media with the electromagnetic waves, The waves essentially get coupled to an oscillator! So I implemented this in a manner not too different from the wavelength sensors (in fact the simulation runs in the alpha of the float4s for the RGB/LMS sensors). Each pixel of transparent media has an associated spring damper which can push back on the wave. The stiffness/damping varies depending on how strong the effect of dispersion is. This is not really implemented in a totally physically accurate way (the oscillators are coupled to the wave linearly), but its similar enough to permittivity that you can produce dispersion.
<br>
<br>
The end result I think is really cool to look at and play around with. One thing that I find super interesting is that at this scale white light is really weird. The "white intensity" slider essentially controls how much white noise is in the signal used to "pump" the light source. At full strength the light source is moving with full randomness, which essentially means energy is spread across all frequencies. This is like thermal vibration! In terms of the visualization this leads to pulses of random rainbow color. If you want to see something actually white you need to turn the "averaging filter constant" way down to it's minimum value. When you do this the colors across many frames are averaged. I wonder if this is somehow related to the kind of color noise your eye picks up when it's really dark. Another interesting thing is if you try moving objects or the light source more than 1 pixel per frame, you actually exceed the speed of light and get some high energy artifacts, this is why there's a "source speed" slider, changing x/y was causing some simulation killing artifacts! You also get red shift/blue shift from the source where the waves bunch up/stretch out when you move it! You also see that "laser beams" diverge and can't stay focused, with this divergence being frequency dependent. For white light this manifests as an airy disk!. I hope you enjoy finding these little links to physical phenomena in the simulator, thanks for reading if you got this far!
	<br>
	<br>
	<hr>
<h2>Site Navigation</h2>
<a href="../index.html">Homepage</a>
	<br>
	<br>
	<hr>
<div id="copyright_text">
&copy; 2020 Kevin Bergamin.
</div>
</div>
<!-- IE/EDGE want this in body... -->
<script id="2d-vertex-shader" type="x-shader/x-vertex">

  // an attribute will receive data from a buffer
  uniform mat3 u_matrix;
  
  attribute vec4 a_position;
  attribute vec2 a_texcoord;
  
  varying vec4 p_position;
  varying vec2 v_texcoord;
  // all shaders have a main function
  void main() {

    // gl_Position is a special variable a vertex shader
    // is responsible for setting
    p_position = a_position;
	p_position.xy = (u_matrix * vec3(p_position.xy, 1)).xy;
    gl_Position = p_position;
    
    // Pass the texcoord to the fragment shader.
    v_texcoord = a_texcoord;
  }

</script>
<script id="2d-fragment-shader" type="notjs">

// fragment shaders don't have a default precision so we need
// to pick one. mediump is a good default
// some fragment shader
#ifdef GL_FRAGMENT_PRECISION_HIGH
  precision highp float;
#else
  precision mediump float;
#endif

uniform vec4 slider4;
uniform float t;
varying vec4 p_position;
varying vec2 v_texcoord;
uniform vec2 pix_sz;
uniform sampler2D u_texture_bg;
uniform sampler2D u_texture;
uniform sampler2D u_texture_prev;
uniform vec4 cfloats;
#define PI 3.1415926535897932384626433832795
/*
void main() {
   
	vec2 s = pix_sz*0.9;
    vec4 t0 = texture2D(u_texture_prev, v_texcoord);
    vec4 t1 = texture2D(u_texture, v_texcoord);
    vec4 c = (texture2D(u_texture, v_texcoord+vec2(s.x,0.0))
                   +texture2D(u_texture, v_texcoord+vec2(-s.x,0.0))
                   +texture2D(u_texture, v_texcoord+vec2(0.0,s.y))
                   +texture2D(u_texture, v_texcoord+vec2(0.0,-s.y)))/2.0-t0;
    
    
    vec4 v = t1-t0;
    gl_FragColor.r = c.r*0.99;
    gl_FragColor.g = 0.0;//v.r*100.0;
    gl_FragColor.b = 0.0;//v.r*-100.0;
    gl_FragColor.a = 1.0;
}*/
float modI(float a,float b) {
    float m=a-floor((a+0.5)/b)*b;
    return floor(m+0.5);
}
void main() {

    vec4 bg = texture2D(u_texture_bg, v_texcoord);
    
    vec4 t0 = texture2D(u_texture_prev, v_texcoord);
    vec4 t1 = texture2D(u_texture, v_texcoord);
    vec4 v = t1-t0;
    
    //the actual time step should never be used since it is so small and will kill precision, work around this by working in terms of dt rather than seconds (e.g. vels = nm/dt)
    //speed of light is assumed c grid cells per timestep
    float c_cell_per_dt = 0.5*slider4.x;
    const vec3 rgb_peak_perceptive_wavelength_nm = vec3(553.71685738,531.2838817,443.52513556);
    const vec3 rgb_eye_sensitivity_damp_ratio = vec3(0.09500278,0.08503587,0.06160486);
    const vec3 rgb_eye_sensitivity_masses = vec3(1.1675646,1.3637187,1.8458749);
    float grid_nm_per_cell = slider4.z; //arbitrary value which we could control
    vec3 rgb_peak_perceptive_waves_per_dt = (c_cell_per_dt*grid_nm_per_cell)/rgb_peak_perceptive_wavelength_nm;
    vec3 rgb_omega = 2.0*PI*rgb_peak_perceptive_waves_per_dt;
    
    //wave speed but not really
    //speed of light*(delta t)/(delta x) = R
    float R = c_cell_per_dt*(1.0-(1.0-bg.g)*cfloats.z);//+0.01*(1.0-bg.g)*t1.g/length(t1.gba+0.0000000000001);
    
    float r_floor =  floor(bg.r-0.5)+1.0;
    float k = mix(0.9,0.9,r_floor);
    float k_ratio = mix(1.0,0.5+cfloats.y*0.5,r_floor);
    float damp_ratio = mix(0.3,0.0,r_floor);
    //const vec3 rgb_frequencies = vec3(9.0,10.5,12.0)/32.0;
        
    if((modI(floor((p_position.y*0.5+0.5)/pix_sz.y),2.00))<0.5){ //halve the resolution to get twice the data fields per sim cell (4 floats isn't enough)
        k = 0.9;
        vec4 p1 = texture2D(u_texture, v_texcoord+vec2(0.0,pix_sz.y));
        gl_FragColor.gba = t1.gba+v.gba-rgb_eye_sensitivity_damp_ratio*rgb_omega*v.gba-rgb_omega*rgb_omega*(t1.gba)+k*p1.r/rgb_eye_sensitivity_masses;
    }else{
        vec4 laplacian_t1 = (texture2D(u_texture, v_texcoord+vec2(pix_sz.x,0.0))
                            +texture2D(u_texture, v_texcoord+vec2(-pix_sz.x,0.0))
                            +texture2D(u_texture, v_texcoord+vec2(0.0,pix_sz.y*2.0))
                            +texture2D(u_texture, v_texcoord+vec2(0.0,-pix_sz.y*2.0)))-4.0*t1;                        
        //laplacian is approximated by the integral of values on the surface of a ball surrounding a given point
        //here we take the values on the circle of radius one pixel centered on the current pixel, and weight by the arc length in the grid cells
        //divided by the surface area of that ball, minus the value at the center. Equal in the limit of a infinitely small ball.
        // we integrate the wave equation using the semi-implicit Euler method
        gl_FragColor.r = (t1.r+v.r*slider4.y*bg.b+R*R*laplacian_t1.r); //symplectic wave equation step
        if(bg.b>0.5){
            gl_FragColor.r-=k*((t1.r-t1.g)+(t1.r-t1.b)+(t1.r-t1.a))*(1.0-k_ratio)*r_floor; //elastic coupling between medium and wave
        }
        gl_FragColor.gba = t1.gba+v.gba-damp_ratio*rgb_omega*v.gba-rgb_omega*rgb_omega*(t1.gba)+k*(vec3(t1.r,t1.r,t1.r)-t1.gba)*k_ratio*r_floor;
    }
    

    /*vec4 laplacian_t1 = (texture2D(u_texture, v_texcoord+vec2(s.x,0.0))*1.04724835653
                        +texture2D(u_texture, v_texcoord+vec2(-s.x,0.0))*1.04724835653
                        +texture2D(u_texture, v_texcoord+vec2(-s.x,-s.y))*0.523547970265
                        +texture2D(u_texture, v_texcoord+vec2(s.x,-s.y))*0.523547970265
                        +texture2D(u_texture, v_texcoord+vec2(-s.x,s.y))*0.523547970265
                        +texture2D(u_texture, v_texcoord+vec2(s.x,s.y))*0.523547970265
                        +texture2D(u_texture, v_texcoord+vec2(0.0,s.y))*1.04724835653
                        +texture2D(u_texture, v_texcoord+vec2(0.0,-s.y))*1.04724835653)/(2.0*PI)-t1;*/

    

   
    

    //gl_FragColor.g = v.r*2.0;
    //these store the excitation of the rgb sensors which are a mass spring dampers which oscillate at the color they display
    
}

</script>
<script id="2d-fragment-shader-basic" type="notjs">

#ifdef GL_FRAGMENT_PRECISION_HIGH
  precision highp float;
#else
  precision mediump float;
#endif

uniform vec4 slider4;
uniform float t;
varying vec4 p_position;
varying vec2 v_texcoord;
uniform vec2 pix_sz;
uniform sampler2D u_texture;
uniform sampler2D u_texture_prev;
uniform vec4 cfloats;
#define PI 3.1415926535897932384626433832795
float rand(float n){return fract(sin(n) * 43758.5453123);}

void main() {
    vec4 t1 = texture2D(u_texture, p_position.xy*0.5+0.5);
    gl_FragColor.r = t;
    if(length(v_texcoord-vec2(0.5,0.5))>=0.5){
        discard;
    }
}
</script>
<script id="2d-fragment-shader-laser" type="notjs">

#ifdef GL_FRAGMENT_PRECISION_HIGH
  precision highp float;
#else
  precision mediump float;
#endif

uniform vec4 slider4;
uniform float t;
varying vec4 p_position;
varying vec2 v_texcoord;
uniform vec2 pix_sz;
uniform sampler2D u_texture;
uniform sampler2D u_texture_prev;
uniform vec4 cfloats;
#define PI 3.1415926535897932384626433832795
float rand(float n){return fract(sin(n) * 43758.5453123);}

void main() {
    vec4 t1 = texture2D(u_texture, p_position.xy*0.5+0.5);
    gl_FragColor.r = t;
    if(v_texcoord.x>0.9 && abs(v_texcoord.y-0.5)<0.4){

    }else{
        gl_FragColor.r =0.0;
    }
}
</script>

<script id="2d-fragment-shader-wave" type="notjs">

#ifdef GL_FRAGMENT_PRECISION_HIGH
  precision highp float;
#else
  precision mediump float;
#endif

uniform vec4 slider4;
uniform float t;
varying vec4 p_position;
varying vec2 v_texcoord;
uniform vec2 pix_sz;
uniform sampler2D u_texture_bg;
uniform sampler2D u_texture;
uniform sampler2D u_texture_prev;
uniform vec4 cfloats;
#define PI 3.1415926535897932384626433832795
float modI(float a,float b) {
    float m=a-floor((a+0.5)/b)*b;
    return floor(m+0.5);
}

//do gabor transform
void main() {
    
    
    vec2 p = pix_sz;
    const int steps = 1;
    const int skipfac = 1;
    const float gaussian_factor = -PI*(2.0/float(steps*steps)); //this gives approximately the right fitting gaussian to the window given by steps
    /*
    vec3 wavelet_s_x = vec3(0.0,0.0,0.0);
    vec3 wavelet_c_x = vec3(0.0,0.0,0.0);
    vec3 wavelet_s_y = vec3(0.0,0.0,0.0);
    vec3 wavelet_c_y = vec3(0.0,0.0,0.0);
    const vec3 rgb_frequencies = vec3(5.0,13.0,18.0)/float(steps);
    */
    /*
    for(int i = -steps; i < steps; i++)
    {
        float x = float(i);
        float g = exp(x*x*gaussian_factor);
        vec3 omega_factor = rgb_frequencies*x;
        s = texture2D(u_texture, v_texcoord+vec2(x*p.x,0.0)).r;
        wavelet_s_x += s*g*sin(omega_factor);
        wavelet_c_x += s*g*cos(omega_factor);
    }
    for(int j = -steps; j < steps; j++)
    {
        float y = float(j);
        float g = exp(y*y*gaussian_factor);
        vec3 omega_factor = rgb_frequencies*y;
        s = texture2D(u_texture, v_texcoord+vec2(0.0,y*p.y)).r;
        wavelet_s_y += s*g*sin(omega_factor);
        wavelet_c_y += s*g*cos(omega_factor);
    }
    gl_FragColor.rgb = (wavelet_s_y*wavelet_s_y+wavelet_c_y*wavelet_c_y)*0.01;
    //gl_FragColor.rgb = (wavelet_s_x*wavelet_s_x+wavelet_c_x*wavelet_c_x+wavelet_s_y*wavelet_s_y+wavelet_c_y*wavelet_c_y)*0.01;
    */
    /*
    for(int j = -steps; j < steps; j+=skipfac)
    {
        for(int i = -steps; i < steps; i+=skipfac)
        {
            float x = float(i);
            float y = float(j);
            float r = length(vec2(x,y));
            float g = exp(r*r*gaussian_factor);
            vec3 omega_factor = rgb_frequencies*r;
            vec4 s = texture2D(u_texture, v_texcoord+vec2(x*p.x,y*p.y));
            wavelet_s_x += abs(s.r)*g*cos(omega_factor);
            wavelet_c_x += abs(s.g)*g*cos(omega_factor);
        }
    }
    gl_FragColor.rgb = (wavelet_s_x*wavelet_s_x+wavelet_c_x*wavelet_c_x)*vec3(1.0,1.0,1.0)*0.001;
    */
    //gl_FragColor.r = abs(texture2D(u_texture, v_texcoord).r);
    //gl_FragColor.g = abs(texture2D(u_texture, v_texcoord).g);
    /*vec4 t0 = texture2D(u_texture_prev, v_texcoord);
    vec4 t1 = texture2D(u_texture, v_texcoord);
    vec4 v = t1-t0;
    
    //ultra strong signal can be made like this

    //gl_FragColor.r += 0.5;
    //gl_FragColor = vec4(1,0,0,1);
    //gl_FragColor.rgb = (wavelet_s_y*wavelet_s_y+wavelet_c_y*wavelet_c_y)*0.01;
    //gl_FragColor.b = 0.0;
    const vec3 rgb_frequencies = vec3(9.0,10.0,13.0)/32.0;
    //gl_FragColor.r = t1.r;
    gl_FragColor.rgb = (0.5*(v.gba*v.gba)+0.5*rgb_frequencies*rgb_frequencies*t1.gba*t1.gba)/100.0;
    //gl_FragColor.r;
    
    //gl_FragColor.rgb *= clamp((texture2D(u_texture, v_texcoord).r)*0.1,0.0,1.0);
    */
    //the actual time step should never be used since it is so small and will kill precision, work around this by working in terms of dt rather than seconds (e.g. vels = nm/dt)
    //speed of light is assumed c grid cells per timestep
    float c_cell_per_dt = 0.5*slider4.x;
    const vec3 rgb_peak_perceptive_wavelength_nm = vec3(553.71685738,531.2838817,443.52513556);
    const vec3 rgb_eye_sensitivity_damp_ratio = vec3(0.09500278,0.08503587,0.06160486);
    const vec3 rgb_eye_sensitivity_masses = vec3(1.1675646,1.3637187,1.8458749);
    float grid_nm_per_cell = slider4.z; //arbitrary value which we could control
    vec3 rgb_peak_perceptive_waves_per_dt = (c_cell_per_dt*grid_nm_per_cell)/rgb_peak_perceptive_wavelength_nm;
    vec3 rgb_omega = 2.0*PI*rgb_peak_perceptive_waves_per_dt;
    
    
    vec4 t0 = texture2D(u_texture_prev, v_texcoord);
    vec4 t1 = texture2D(u_texture, v_texcoord);
    vec4 v = t1-t0;
    //const vec3 rgb_frequencies = vec3(9.0,10.5,12.0)/32.0;
    //gl_FragColor.rgb = 0.5*((v.gba*v.gba)+rgb_frequencies*rgb_frequencies*(t1.gba*t1.gba-v.gba*t1.gba))/100.0;
    gl_FragColor.a = 1.0;
    vec3 integrator = vec3(0.0,0.0,0.0);
    float fac_area = 0.0;
    if((modI(floor((p_position.y*0.5+0.5)/pix_sz.y),2.00))<0.5){
        for(int j = -steps; j < steps; j+=skipfac)
            {
                for(int i = -steps; i < steps; i+=skipfac)
                {
                    float x = float(i);
                    float y = float(j);
                    float r = length(vec2(x,y));
                    float g = exp(r*r*gaussian_factor);
                    vec3 omega_factor = rgb_omega*r;
                    vec4 s1 = texture2D(u_texture, v_texcoord+vec2(x*p.x,y*p.y));
                    vec4 s0 = texture2D(u_texture_prev, v_texcoord+vec2(x*p.x,y*p.y));
                    vec4 v = s1-s0;
                    integrator += 0.5*(rgb_eye_sensitivity_masses*((v.gba*v.gba)+rgb_omega*rgb_omega*(s1.gba*s1.gba-v.gba*s1.gba)))/0.5*rgb_eye_sensitivity_damp_ratio;
                    fac_area += g;
                }
            }
            vec4 c = texture2D(u_texture_bg, v_texcoord);
            gl_FragColor.rgb = mix(c.rgb,(((integrator/fac_area)*1.0)),cfloats.x);
            //vec4 p1 = texture2D(u_texture, v_texcoord+(0.0,p.y));
            //gl_FragColor.r = p1.r;//clamp(p1.r,0.0,1.0);
    }

    //
            
            //gl_FragColor.r = t1.g;
    //}else{
    //        vec4 c = texture2D(u_texture_bg, v_texcoord-vec2(0.0,-pix_sz.y));
    //        gl_FragColor.rgb = c.rgb;    
    //        gl_FragColor.b = 1.0;   
    //}
    
    //gl_FragColor.rgb+= clamp(t1.r,0.0,1.0)*0.1;
    //gl_FragColor= t1/length(t1.gba+0.0000000000001);
    //gl_FragColor.a = 1.0;
}

</script>
<script id="2d-fragment-shader-wave-avg" type="notjs">

#ifdef GL_FRAGMENT_PRECISION_HIGH
  precision highp float;
#else
  precision mediump float;
#endif

uniform vec4 slider4;
uniform float t;
varying vec4 p_position;
varying vec2 v_texcoord;
uniform vec2 pix_sz;
uniform sampler2D u_texture;
uniform sampler2D u_texture_prev;
uniform vec4 cfloats;
#define PI 3.1415926535897932384626433832795
float modI(float a,float b) {
    float m=a-floor((a+0.5)/b)*b;
    return floor(m+0.5);
}
vec3 XYZ_2_RGB(vec3 C) {
  vec3 RGB = vec3(
 3.2404542*C.x - 1.5371385*C.y - 0.4985314*C.z,
-0.9692660*C.x + 1.8760108*C.y + 0.0415560*C.z,
 0.0556434*C.x - 0.2040259*C.y + 1.0572252*C.z);
  //sRGB correction?
  vec3 abs_RGB = abs(RGB);
  return clamp(mix(1.055 * pow(RGB, vec3(0.41666,0.41666,0.41666)) - 0.055, 12.92 * RGB,vec3(abs_RGB.x<0.0031308,abs_RGB.y<0.0031308,abs_RGB.z<0.0031308)),0.0,1.0);
  //return RGB;
}
vec3 LMS_2_RGB(vec3 LMS) {
    vec3 XYZ = vec3(
        1.91019683*LMS.x-1.11212389*LMS.y+2.01907957e-01*LMS.z,
        3.70950088e-01*LMS.x+6.29054257e-01*LMS.y-8.05514218e-06*LMS.z,
        1.00000000*LMS.z
    );
    return XYZ_2_RGB(XYZ);
}
//average values over time.
void main() {
    if((modI(floor((p_position.y*0.5+0.5)/pix_sz.y),2.00))<0.5){
        vec4 s1 = texture2D(u_texture, v_texcoord);
        gl_FragColor.rgb = LMS_2_RGB(s1.rgb)*1.0;
    }else{
        vec4 s1 = texture2D(u_texture, v_texcoord-vec2(0.0,pix_sz.y));
        gl_FragColor.rgb = LMS_2_RGB(s1.rgb)*1.0;    
    }
    gl_FragColor.a = 1.0;
}

</script>
<script src="js/webgl-utils.js"></script>
<script src="js/m3.js"></script>
<script src="js/main.js"></script>
</body>