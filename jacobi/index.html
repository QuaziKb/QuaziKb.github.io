<html>
<head>
	<link rel="stylesheet" type="text/css" href="../index.css">
	<META name="keywords" content="matrix, linear, algebra, interactive, jacobi, algorithm"> 
	<title>Jacobi Algorithm</title>
		<style>
			body { margin: 0; }
			canvas { width: 100%; height: 100% }
		</style>
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function () {	
  // Hide MathJax until rendered by putting them in divs of the class "hide_math"
	var mathdivs = document.getElementsByClassName("hide_math");
	for (i = 0; i < mathdivs.length; i++) { 
		mathdivs[i].style.visibility = "";
	}	
  });
</script>
<script 
   src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'>
</script>
<script src="../js/QZMath.js"></script>
<script src="../js/three.js"></script>
<script src="../js/OrbitControls.js"></script>	
</head>
<body>
<div id="wrap">

<h1>2x2 Jacobi Algorithm</h1>
<br>
Finding eigenvalues and eigenvectors of symmetric matrices is pretty useful. Typically you don't have to implement your own algorithms to do this since there are so many packages around that can do it really well. But there are still times you might want to do it yourself, particularly when dealing with small matrices. For 2x2 matrices there's a fast analytic approach, and this can be leveraged to iteratively converge for "larger" matrices. The technique I'm going to use here is called the "Jacobi Algorithm". I used <a href = http://people.math.gatech.edu/~klounici6/2605/Lectures%20notes%20Carlen/chap3.pdf>Chapter 3 of Calculus++: The Symmetric Eigenvalue Problem</a> by Eric A Carlen to figure out how to get this working.
<br>
<br>
for such a matrix I show how eigenvalues \(\mu_+\) and \(\mu_-\) are found,
<br>
<br>
<div class="hide_math" style="visibility:hidden">
	\[
	\begin{align}
	A={A}^\intercal&=\begin{bmatrix}a & b\\b & d\end{bmatrix}\\
	\mu_+&=\dfrac{a+d}{2}+\sqrt{b^2+(\dfrac{a-d}{2})^2}\\
	\mu_-&=\dfrac{a+d}{2}-\sqrt{b^2+(\dfrac{a-d}{2})^2}\\
	\end{align}
	\]
</div>
<br>
<br>
<div class="">
with the eigenvalues it's easy to find the eigenvectors \(\textbf{u}_0\) and \(\textbf{u}_1\),
</div>
<br>
<br>
<div class="hide_math" style="visibility:hidden">
	<p>
	\begin{align}
	\begin{bmatrix}\textbf{r}_0\\\textbf{r}_1\end{bmatrix}&=A-I\mu_+\\
	\textbf{u}_0&=\dfrac{1}{||\textbf{r}_0||}\textbf{r}_0^\perp
	\end{align}
	</p>
</div>
<br>
<br>
<div class="">
finding \(\textbf{v}^\perp\) from \(\textbf{v}\) is straightforward in 2D,
</div>
<br>
<br>
<div class="hide_math" style="visibility:hidden">
	<p>
	\begin{align}
	\textbf{v}&=\begin{bmatrix}v_0\\v_1\end{bmatrix}\\
	\textbf{v}^\perp&=\begin{bmatrix}-v_1\\v_0\end{bmatrix}\\
	\end{align}
	</p>
</div>
<br>
<br>
<div class="">
for symmetric matrices all eigenvectors are orthogonal so : \(\textbf{u}_1=\textbf{u}_0^\perp\). With all this we can perform a diagonalization:
</div>
<br>
<br>
<div class="hide_math" style="visibility:hidden">
	<p>
	\begin{align}
	A&={A}^\intercal\\
	U&=\begin{bmatrix}\textbf{u}_0 & \textbf{u}_1\end{bmatrix}\\
	\Lambda = {U}^\intercal AU&=\begin{bmatrix}\mu_+ & \\ & \mu_-\end{bmatrix}\\
	A&=U\Lambda{U}^\intercal\\
	\end{align}
	</p>
</div>
<br>
<br>
<div>
Below you can modify some arbitrary \(A\) matrix and see the resulting orthogonal eigenvector matrix \(U\) and the result of \({U}^\intercal AU\), which should be diagonal \(\Lambda\) if  \(A={A}^\intercal\).
</div>
<br>
<br>
<div class="hide_math centered" style="visibility:hidden">
	<table class="borderless_table">
	<tr>
		<td style="text-align:right;">\(\LARGE A = \)</td>
		<td>
			<div id="A_mtx" class="bracketed"></div>
		</td> 
	</tr>
	<tr>
		<td style="text-align:right;">\(\LARGE U = \)</td>
		<td>
			<div id="U_mtx" class="bracketed"></div>
		</td> 
	</tr>
	<tr>
		<td style="text-align:right;">\(\LARGE \Lambda = {U}^\intercal AU = \)</td>
		<td>
			<div id="UtAU_mtx" class="bracketed"></div>
		</td> 
	</tr>
	</table>
</div>
<script src="js/main.js"></script>
<br>
<div id="cent_text">
<i><b>figure 1</b> : modify matrix A by typing in entries.</i>
</div>
<br>
<br>
<div>
Some practical notes when you implement this. If the off diagonal elements are zero, and the diagonal elements are equal or one is greater than the other, the process outlined above seems to spit out NaN's due to a division by zero. I found an easy way to fix this is to just add some very small value to the off diagonals to ensure they're non-zero. This prevents the division by zero. Technically there's no point even doing anything in this case because your eigenvalues ARE the diagonal entries, however maybe it's advantageous to code this without adding conditional branching for performance reasons on some hardware (GPU implementations?).
<br>
<br>
Check out <a href="../jacobi3/index.html">3x3 jacobi algorithm</a> for an example of using the algorithm to iteratively diagonalize in the 3x3 case.
</div>
<br>
<br>
<hr>
<h2>Site Navigation</h2>
<a href="../index.html">Homepage</a>
	<br>
	<br>
	<hr>
<div id="copyright_text">
&copy; 2018 Kevin Bergamin.
</div>
</div>

</BODY>
</HTML>
